{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak4nksha/duplicate-bug-detector/blob/main/notebooks/03_model_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHvpGmNEgk0h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q rank-bm25 sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "iq7Z0QqA6y49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WAyk2_IphKqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"/content/drive/MyDrive/DuplicateBugsDetector/cleaned_files\"\n",
        "train_df = pd.read_csv(f\"{DATA_ROOT}/train.csv\")\n",
        "test_df  = pd.read_csv(f\"{DATA_ROOT}/test.csv\")\n",
        "\n",
        "print(train_df.shape, test_df.shape)\n",
        "train_df.head(2)"
      ],
      "metadata": {
        "id": "TN4xlfi7hN5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation helper functions\n",
        "\n",
        "def first_relevant_rank(scores, train_group, target_group):\n",
        "    sorted_idx = np.argsort(-scores)\n",
        "    for rank, idx in enumerate(sorted_idx, 1):\n",
        "        if train_group[idx] == target_group:\n",
        "            return rank\n",
        "    return np.inf\n",
        "\n",
        "def recall_at_k(ranks, k):\n",
        "    ranks = np.array(ranks)\n",
        "    return np.mean(ranks <= k)\n",
        "\n",
        "def mrr(ranks):\n",
        "    ranks = np.array(ranks)\n",
        "    return np.mean(1 / ranks)"
      ],
      "metadata": {
        "id": "F6KvY-sA6bSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tf-idf baseline for comparision\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    min_df=5,\n",
        "    max_df=0.9,\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "X_train = tfidf.fit_transform(train_df[\"text\"].fillna(\"\"))\n",
        "\n",
        "train_ids   = train_df[\"issue_id\"].astype(str).to_numpy()\n",
        "train_group = train_df[\"dup_group\"]\n",
        "#print(tfidf.get_params())\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "JfhsMW6-7IFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we evaluate only queries that actually have a duplicate group\n",
        "qset = test_df[test_df[\"dup_group\"].notna()].copy()\n",
        "qset = qset.reset_index(drop=True)\n",
        "\n",
        "X_test = tfidf.transform(qset[\"text\"].fillna(\"\"))\n",
        "\n",
        "ranks_overall = []\n",
        "ranks_by_project = {p: [] for p in qset[\"project\"].unique()}\n",
        "\n",
        "print(\"Running TF–IDF retrieval for\", len(qset), \"queries...\")\n",
        "\n",
        "for i, row in qset.iterrows():\n",
        "    query_vec = X_test[i]\n",
        "    scores = query_vec.dot(X_train.T).toarray().ravel()\n",
        "\n",
        "    r = first_relevant_rank(\n",
        "        scores,\n",
        "        train_group=train_group,\n",
        "        target_group=row[\"dup_group\"],\n",
        "    )\n",
        "\n",
        "    ranks_overall.append(r)\n",
        "    ranks_by_project[row[\"project\"]].append(r)\n"
      ],
      "metadata": {
        "id": "IUgfzYnICRAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summaries for baseline\n",
        "\n",
        "def summarize(ranks):\n",
        "    ranks = np.array(ranks)\n",
        "    return {\n",
        "        \"R@1\":  recall_at_k(ranks, 1),\n",
        "        \"R@5\":  recall_at_k(ranks, 5),\n",
        "        \"R@10\": recall_at_k(ranks, 10),\n",
        "        \"MRR\":  mrr(ranks),\n",
        "    }\n",
        "\n",
        "overall = summarize(ranks_overall)\n",
        "project_stats = {p: summarize(r) for p, r in ranks_by_project.items()}\n",
        "\n",
        "tfidf_metrics = pd.DataFrame(\n",
        "    [overall] + list(project_stats.values()),\n",
        "    index=[\"OVERALL\"] + list(project_stats.keys()),\n",
        "    columns=[\"R@1\", \"R@5\", \"R@10\", \"MRR\"],\n",
        ")\n",
        "\n",
        "print(\"\\nTF–IDF Retrieval Metrics:\")\n",
        "print(tfidf_metrics)"
      ],
      "metadata": {
        "id": "tvYjwksAE0uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BM25 model"
      ],
      "metadata": {
        "id": "HFT0-AVvGRiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict\n",
        "\n",
        "# def simple_tokenize(text):\n",
        "#     if not isinstance(text, str):\n",
        "#         return []\n",
        "#     return text.lower().split()\n",
        "\n",
        "# #corpus for BM25\n",
        "# train_corpus = [simple_tokenize(doc) for doc in train_df[\"text\"].fillna(\"\")]\n",
        "# bm25 = BM25Okapi(train_corpus)\n",
        "\n",
        "# qset = test_df[test_df[\"dup_group\"].notna()].copy()\n",
        "# q_tokens = [simple_tokenize(t) for t in qset[\"text\"].fillna(\"\")]\n",
        "\n",
        "# ranks_overall_bm25 = []\n",
        "# ranks_by_project_bm25 = {p: [] for p in qset[\"project\"].unique()}\n",
        "\n",
        "# print(\"Running BM25 retrieval for\", len(qset), \"queries...\")\n",
        "\n",
        "# for (idx, row), query_tokens in zip(qset.iterrows(), q_tokens):\n",
        "#     scores = bm25.get_scores(query_tokens)       # len = n_train\n",
        "#     r = first_relevant_rank(\n",
        "#         np.array(scores),\n",
        "#         train_group=train_group,\n",
        "#         target_group=row[\"dup_group\"],\n",
        "#     )\n",
        "#     ranks_overall_bm25.append(r)\n",
        "#     ranks_by_project_bm25[row[\"project\"]].append(r)\n"
      ],
      "metadata": {
        "id": "DKHaLbpKMv41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Summaries for bm25\n",
        "\n",
        "# overall_bm25 = summarize(ranks_overall_bm25)\n",
        "# project_bm25 = {p: summarize(r) for p, r in ranks_by_project_bm25.items()}\n",
        "\n",
        "# bm25_metrics = pd.DataFrame(\n",
        "#     [overall_bm25] + list(project_bm25.values()),\n",
        "#     index=[\"OVERALL\"] + list(project_bm25.keys()),\n",
        "#     columns=[\"R@1\", \"R@5\", \"R@10\", \"MRR\"],\n",
        "# )\n",
        "\n",
        "# print(\"\\nBM25 Retrieval Metrics:\")\n",
        "# print(bm25_metrics)\n"
      ],
      "metadata": {
        "id": "XzBGWaDGH6wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"bm25s[full]\"\n",
        "\n",
        "import bm25s\n",
        "import numpy as np\n",
        "\n",
        "corpus = train_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "print(\"Number of docs in train_df:\", len(corpus))\n",
        "\n",
        "corpus_tokens = [doc.lower().split() for doc in corpus]\n",
        "print(\"Number of tokenized docs:\", len(corpus_tokens))\n",
        "\n",
        "bm25_retriever = bm25s.BM25()          # Lucene-style BM25 by default\n",
        "bm25_retriever.index(corpus_tokens)    # index the tokenized documents\n",
        "\n",
        "print(\"BM25S index built on\", len(corpus_tokens), \"documents.\")\n"
      ],
      "metadata": {
        "id": "knUZpEG-JbHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks_overall_bm25 = []\n",
        "ranks_by_project_bm25 = {p: [] for p in qset[\"project\"].unique()}\n",
        "\n",
        "print(\"Running BM25S retrieval for\", len(qset), \"queries...\")\n",
        "\n",
        "for _, row in qset.iterrows():\n",
        "    query_text = row[\"text\"] if isinstance(row[\"text\"], str) else \"\"\n",
        "    query_tokens = query_text.lower().split()\n",
        "\n",
        "    scores = np.array(bm25_retriever.get_scores(query_tokens))  # shape: (n_train,)\n",
        "\n",
        "    r = first_relevant_rank(\n",
        "        scores,\n",
        "        train_group=train_group,\n",
        "        target_group=row[\"dup_group\"],\n",
        "    )\n",
        "\n",
        "    ranks_overall_bm25.append(r)\n",
        "    ranks_by_project_bm25[row[\"project\"]].append(r)\n",
        "\n",
        "# Summaries for BM25\n",
        "overall_bm25 = summarize(ranks_overall_bm25)\n",
        "project_bm25 = {}\n",
        "\n",
        "for project in ranks_by_project_bm25:\n",
        "    ranks = ranks_by_project_bm25[project]\n",
        "    metrics = summarize(ranks)\n",
        "    project_bm25[project] = metrics\n",
        "\n",
        "bm25s_metrics = pd.DataFrame(\n",
        "    [overall_bm25] + list(project_bm25.values()),\n",
        "    index=[\"OVERALL\"] + list(project_bm25.keys()),\n",
        "    columns=[\"R@1\", \"R@5\", \"R@10\", \"MRR\"],\n",
        ")\n",
        "\n",
        "print(\"\\nBM25S Retrieval Metrics:\")\n",
        "print(bm25s_metrics)"
      ],
      "metadata": {
        "id": "caGacKdJX8a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SBERT Model"
      ],
      "metadata": {
        "id": "Oy4Nf4_JrpUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SBERT model – building embeddings for all train documents\n",
        "\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lyeBOEwZrx56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "4I17TJAAr0Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbert_model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# texts for the index\n",
        "train_texts = train_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "train_embs = sbert_model.encode(\n",
        "    train_texts,\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,   # so cosine similarity = dot product\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "print(\"Train SBERT embeddings shape:\", train_embs.shape)\n"
      ],
      "metadata": {
        "id": "KLZsfx3Lrro9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SBERT retrieval and evaluation\n",
        "\n",
        "# only queries that actually have a dup_group\n",
        "print(\"SBERT: queries with ground truth duplicates:\", len(qset))\n",
        "\n",
        "query_texts = qset[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "query_embs = sbert_model.encode(\n",
        "    query_texts,\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,   # keep unit vectors\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "ranks_overall_sbert = []\n",
        "ranks_by_project_sbert = {p: [] for p in qset[\"project\"].unique()}\n",
        "\n",
        "train_group = train_df[\"dup_group\"].to_numpy()\n",
        "\n",
        "for i, (_, row) in enumerate(qset.iterrows()):\n",
        "    q_emb = query_embs[i]\n",
        "    scores = train_embs @ q_emb    # cosine via dot product\n",
        "\n",
        "    r = first_relevant_rank(\n",
        "        scores,\n",
        "        train_group=train_group,\n",
        "        target_group=row[\"dup_group\"],\n",
        "    )\n",
        "\n",
        "    ranks_overall_sbert.append(r)\n",
        "    ranks_by_project_sbert[row[\"project\"]].append(r)\n",
        "\n",
        "\n",
        "overall_sbert = summarize(ranks_overall_sbert)\n",
        "\n",
        "project_sbert = {}\n",
        "for project, ranks in ranks_by_project_sbert.items():\n",
        "    metrics = summarize(ranks)\n",
        "    project_sbert[project] = metrics\n",
        "\n",
        "sbert_metrics = pd.DataFrame(\n",
        "    [overall_sbert] + list(project_sbert.values()),\n",
        "    index=[\"OVERALL\"] + list(project_sbert.keys()),\n",
        "    columns=[\"R@1\", \"R@5\", \"R@10\", \"MRR\"],\n",
        ")\n",
        "\n",
        "print(\"\\nSBERT Retrieval Metrics:\")\n",
        "print(sbert_metrics)\n"
      ],
      "metadata": {
        "id": "538vG0vCt0_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Comparison Table\n",
        "\n",
        "overall_tfidf = tfidf_metrics.loc[\"OVERALL\"]\n",
        "overall_bm25  = bm25s_metrics.loc[\"OVERALL\"]\n",
        "overall_sbert = sbert_metrics.loc[\"OVERALL\"]\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"TF-IDF\": overall_tfidf,\n",
        "    \"BM25S\": overall_bm25,\n",
        "    \"SBERT\": overall_sbert\n",
        "}).T  # transpose so models are rows\n",
        "\n",
        "comparison_df = comparison_df.round(4)\n",
        "\n",
        "print(\"\\n Combined Retrieval Performance (Overall Metrics):\")\n",
        "display(comparison_df)\n"
      ],
      "metadata": {
        "id": "f7EaMoRC-ebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine-Tuned SBERT"
      ],
      "metadata": {
        "id": "WJtgbe0UlQx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ],
      "metadata": {
        "id": "G4kTU1HzlP9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = []\n",
        "grouped = train_df.dropna(subset=[\"dup_group\"]).groupby(\"dup_group\")\n",
        "\n",
        "# Building positive pairs from duplicate groups\n",
        "for group_id, group in grouped:\n",
        "    texts = group[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "    if len(texts) < 2:\n",
        "      continue # need at least a pair\n",
        "\n",
        "    random.shuffle(texts)\n",
        "\n",
        "    # consecutive pairs inside the group\n",
        "    num_texts = len(texts)\n",
        "    for i in range(num_texts - 1):\n",
        "        ex = InputExample(texts=[texts[i], texts[i + 1]])\n",
        "        train_examples.append(ex)\n",
        "\n",
        "    # wrap-around pair (last with first) to add one more positive\n",
        "    if num_texts > 2:\n",
        "        ex = InputExample(texts=[texts[-1], texts[0]])\n",
        "        train_examples.append(ex)\n",
        "\n",
        "print(\"Number of training pairs:\", len(train_examples))"
      ],
      "metadata": {
        "id": "xdYNN4hQlU3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=tuned_model)\n",
        "\n",
        "tuned_model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=2,\n",
        "    warmup_steps=int(len(train_dataloader) * 0.1),\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "# Re-encoding train and query texts with the tuned model\n",
        "tuned_train_embs = tuned_model.encode(\n",
        "    train_texts,\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "tuned_query_embs = tuned_model.encode(\n",
        "    query_texts,\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "print(\"Tuned SBERT embedding shapes:\", tuned_train_embs.shape, tuned_query_embs.shape)"
      ],
      "metadata": {
        "id": "sIsNba4-luma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks_overall_tuned = []\n",
        "ranks_by_project_tuned = {p: [] for p in qset[\"project\"].unique()}\n",
        "\n",
        "train_group_arr = train_df[\"dup_group\"].to_numpy()\n",
        "\n",
        "for i, (_, row) in enumerate(qset.iterrows()):\n",
        "    q_emb = tuned_query_embs[i]\n",
        "    scores = tuned_train_embs @ q_emb   # cosine via dot product\n",
        "\n",
        "    r = first_relevant_rank(\n",
        "        scores,\n",
        "        train_group=train_group_arr,\n",
        "        target_group=row[\"dup_group\"],\n",
        "    )\n",
        "\n",
        "    ranks_overall_tuned.append(r)\n",
        "    ranks_by_project_tuned[row[\"project\"]].append(r)\n",
        "\n",
        "overall_tuned = summarize(ranks_overall_tuned)\n",
        "\n",
        "project_tuned = {}\n",
        "for project, ranks in ranks_by_project_tuned.items():\n",
        "    metrics = summarize(ranks)\n",
        "    project_tuned[project] = metrics\n",
        "\n",
        "tuned_sbert_metrics = pd.DataFrame(\n",
        "    [overall_tuned] + list(project_tuned.values()),\n",
        "    index=[\"OVERALL\"] + list(project_tuned.keys()),\n",
        "    columns=[\"R@1\", \"R@5\", \"R@10\", \"MRR\"],\n",
        ")\n",
        "\n",
        "print(\"\\nFine-tuned SBERT Retrieval Metrics:\")\n",
        "print(tuned_sbert_metrics)\n"
      ],
      "metadata": {
        "id": "NEzOcAOTl8gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_tfidf = tfidf_metrics.loc[\"OVERALL\"]\n",
        "overall_bm25  = bm25s_metrics.loc[\"OVERALL\"]\n",
        "overall_sbert = sbert_metrics.loc[\"OVERALL\"]\n",
        "overall_tuned = tuned_sbert_metrics.loc[\"OVERALL\"]\n",
        "\n",
        "comparison_with_tuned = pd.DataFrame({\n",
        "    \"TF-IDF\":          overall_tfidf,\n",
        "    \"BM25S\":           overall_bm25,\n",
        "    \"SBERT (base)\":    overall_sbert,\n",
        "    \"SBERT (tuned)\":   overall_tuned,\n",
        "}).T.round(4)\n",
        "\n",
        "print(\"\\nCombined Retrieval Performance with Fine-tuned SBERT (Overall Metrics):\")\n",
        "display(comparison_with_tuned)"
      ],
      "metadata": {
        "id": "JzIaI_YLmAGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of duplicate groups with ticket IDs\n",
        "\n",
        "# Filter only rows that have a duplicate group ID\n",
        "dup_df = train_df.dropna(subset=[\"dup_group\"])[[\"issue_id\", \"dup_group\"]]\n",
        "\n",
        "# Count total tickets per group\n",
        "group_sizes = dup_df.groupby(\"dup_group\")[\"issue_id\"].count().sort_values(ascending=False)\n",
        "\n",
        "print(\"Number of duplicate groups:\", len(group_sizes))\n",
        "print(\"\\nOverall distribution of group sizes:\")\n",
        "print(group_sizes.describe())\n",
        "\n",
        "# Show groups with ≥3 tickets (actual duplicate clusters)\n",
        "multi_groups = group_sizes[group_sizes >= 5]\n",
        "\n",
        "print(\"\\nGroups with 3 or more tickets (true duplicate clusters):\")\n",
        "print(multi_groups.head(20))   # show first 20 for preview"
      ],
      "metadata": {
        "id": "w2LtjL8labKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For more detailed inspection: group_id → list of all ticket IDs\n",
        "group_to_ids = dup_df.groupby(\"dup_group\")[\"issue_id\"].apply(list)\n",
        "\n",
        "# Combine size + IDs into one DataFrame\n",
        "dup_dist_df = pd.DataFrame(\n",
        "    {\n",
        "        \"group_size\": group_sizes,\n",
        "        \"ticket_ids\": group_to_ids,\n",
        "    }\n",
        ").sort_values(\"group_size\", ascending=False)\n",
        "\n",
        "print(\"\\nFull duplicate-group distribution with ticket ID lists:\")\n",
        "display(dup_dist_df.head(20))"
      ],
      "metadata": {
        "id": "1AprGSfyahf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df[\"issue_id\"] = train_df[\"issue_id\"].astype(\"Int64\")\n",
        "train_df[\"dup_group\"] = train_df[\"dup_group\"].astype(\"Int64\")\n",
        "\n",
        "dup_df = train_df.dropna(subset=[\"dup_group\"])[[\"issue_id\", \"dup_group\"]]\n",
        "\n",
        "group_sizes = (\n",
        "    dup_df\n",
        "    .groupby(\"dup_group\")[\"issue_id\"]\n",
        "    .count()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "group_to_ids = dup_df.groupby(\"dup_group\")[\"issue_id\"].apply(list)\n",
        "\n",
        "print(\"Number of duplicate groups:\", len(group_sizes))\n",
        "print(\"\\nOverall distribution of group sizes:\")\n",
        "print(group_sizes.describe())\n",
        "\n",
        "#Distribution of duplicates per anchor ticket (size - 1)\n",
        "\n",
        "num_duplicates = group_sizes - 1\n",
        "num_duplicates.name = \"num_duplicates\"\n",
        "\n",
        "print(\"\\nDistribution of number of duplicates per anchor ticket:\")\n",
        "print(num_duplicates.describe())\n",
        "\n",
        "print(\"\\nCounts for groups with 0–24 duplicates (excluding anchor):\")\n",
        "dup_hist = num_duplicates.value_counts().sort_index()\n",
        "#dup_hist = dup_hist[dup_hist.index <= 10]\n",
        "print(dup_hist)\n",
        "\n",
        "\n",
        "example_group = 1942560\n",
        "\n",
        "if example_group in group_to_ids.index:\n",
        "    print(f\"\\nDetails for duplicate group {example_group}:\")\n",
        "    print(\"All ticket IDs in this group:\", group_to_ids.loc[example_group])\n",
        "    print(\"Total tickets in group:\", int(group_sizes.loc[example_group]))\n",
        "    print(\"Number of duplicates (excluding anchor):\", int(num_duplicates.loc[example_group]))\n",
        "else:\n",
        "    print(f\"\\nDuplicate group {example_group} not found in this dataset.\")\n"
      ],
      "metadata": {
        "id": "cfcIA8i34Www"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qset.head())"
      ],
      "metadata": {
        "id": "OVwoTVeI8S9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NA sizes with 0 (these queries have no known duplicate group)\n",
        "qset[\"dup_group_size\"] = qset[\"dup_group\"].map(group_sizes)\n",
        "qset[\"dup_group_size\"] = qset[\"dup_group_size\"].fillna(0).astype(int)\n",
        "\n",
        "# Show query duplicate-group size distribution\n",
        "# print(\"Query group size distribution:\")\n",
        "# print(qset[\"dup_group_size\"].describe())\n",
        "\n",
        "# Count duplicates (size-1)\n",
        "qset_num_duplicates = qset[\"dup_group_size\"] - 1\n",
        "qset_num_duplicates.name = \"num_duplicates\"\n",
        "print(\"\\nCounts of query duplicate-group sizes (0–24 duplicates):\")\n",
        "print(qset_num_duplicates.value_counts().sort_index())\n",
        "\n",
        "# Threshold: groups with >=3 duplicates -> size >=4 total tickets\n",
        "SIZE_THRESHOLD = 4\n",
        "mask_high = qset[\"dup_group_size\"] >= SIZE_THRESHOLD\n",
        "mask_low = qset[\"dup_group_size\"] < SIZE_THRESHOLD\n",
        "\n",
        "print(\"\\nQueries in high-support groups (size >= 4):\", mask_high.sum())\n",
        "print(\"Queries in low-support groups  (size <  4):\", mask_low.sum())\n",
        "\n",
        "# Convert masks to array indices\n",
        "idx_high = np.where(mask_high.to_numpy())[0]\n",
        "#idx_low = np.where(mask_low.to_numpy())[0]\n",
        "\n",
        "def subset_ranks(ranks, idxs):\n",
        "    return [ranks[i] for i in idxs]\n",
        "\n",
        "# Subset ranks for each model\n",
        "tfidf_high = subset_ranks(ranks_overall, idx_high)\n",
        "#tfidf_low = subset_ranks(ranks_overall, idx_low)\n",
        "\n",
        "bm25_high = subset_ranks(ranks_overall_bm25, idx_high)\n",
        "#bm25_low = subset_ranks(ranks_overall_bm25, idx_low)\n",
        "\n",
        "sbert_high = subset_ranks(ranks_overall_sbert, idx_high)\n",
        "#sbert_low = subset_ranks(ranks_overall_sbert, idx_low)\n",
        "\n",
        "tuned_high = subset_ranks(ranks_overall_tuned, idx_high)\n",
        "#tuned_low = subset_ranks(ranks_overall_tuned, idx_low)\n",
        "\n",
        "# Build comparison table\n",
        "results_support = pd.DataFrame({\n",
        "    \"TF-IDF (all)\": pd.Series(summarize(ranks_overall)),\n",
        "    \"TF-IDF (high)\": pd.Series(summarize(tfidf_high)),\n",
        "    #\"TF-IDF (low)\": pd.Series(summarize(tfidf_low)),\n",
        "\n",
        "    \"BM25S (all)\": pd.Series(summarize(ranks_overall_bm25)),\n",
        "    \"BM25S (high)\": pd.Series(summarize(bm25_high)),\n",
        "   # \"BM25S (low)\": pd.Series(summarize(bm25_low)),\n",
        "\n",
        "    \"SBERT base (all)\": pd.Series(summarize(ranks_overall_sbert)),\n",
        "    \"SBERT base (high)\": pd.Series(summarize(sbert_high)),\n",
        "   # \"SBERT base (low)\": pd.Series(summarize(sbert_low)),\n",
        "\n",
        "    \"SBERT tuned (all)\": pd.Series(summarize(ranks_overall_tuned)),\n",
        "    \"SBERT tuned (high)\": pd.Series(summarize(tuned_high)),\n",
        "   # \"SBERT tuned (low)\": pd.Series(summarize(tuned_low)),\n",
        "}).T.round(4)\n",
        "\n",
        "results_support_pct = (results_support * 100).round(1)\n",
        "\n",
        "print(\"Performance by duplicate-group support level in %:\")\n",
        "display(results_support_pct)\n",
        "\n",
        "# print(\"\\nPerformance by duplicate-group support level:\")\n",
        "# display(results_support)\n"
      ],
      "metadata": {
        "id": "YX2feZTRit7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_ranks(ranks, idxs):\n",
        "    return [ranks[i] for i in idxs]\n",
        "\n",
        "# indices for queries that are NOT from Firefox\n",
        "mask_no_firefox = qset[\"project\"] != \"firefox\"\n",
        "idx_no_firefox = np.where(mask_no_firefox.to_numpy())[0]\n",
        "\n",
        "# Overall metrics with all three projects\n",
        "tfidf_all = summarize(ranks_overall)\n",
        "bm25_all = summarize(ranks_overall_bm25)\n",
        "sbert_all = summarize(ranks_overall_sbert)\n",
        "tuned_all = summarize(ranks_overall_tuned)\n",
        "\n",
        "# Overall metrics with only Hadoop + HBase (no Firefox)\n",
        "tfidf_no_ff = summarize(subset_ranks(ranks_overall, idx_no_firefox))\n",
        "bm25_no_ff = summarize(subset_ranks(ranks_overall_bm25, idx_no_firefox))\n",
        "sbert_no_ff = summarize(subset_ranks(ranks_overall_sbert, idx_no_firefox))\n",
        "tuned_no_ff = summarize(subset_ranks(ranks_overall_tuned, idx_no_firefox))\n",
        "\n",
        "results_projects = pd.DataFrame({\n",
        "    \"TF-IDF (all projects)\": pd.Series(tfidf_all),\n",
        "    \"TF-IDF (no Firefox)\":   pd.Series(tfidf_no_ff),\n",
        "\n",
        "    \"BM25S (all projects)\":  pd.Series(bm25_all),\n",
        "    \"BM25S (no Firefox)\":    pd.Series(bm25_no_ff),\n",
        "\n",
        "    \"SBERT base (all)\":      pd.Series(sbert_all),\n",
        "    \"SBERT base (no Firefox)\": pd.Series(sbert_no_ff),\n",
        "\n",
        "    \"SBERT tuned (all)\":     pd.Series(tuned_all),\n",
        "    \"SBERT tuned (no Firefox)\": pd.Series(tuned_no_ff),\n",
        "}).T\n",
        "\n",
        "results_projects_pct = (results_projects * 100).round(1)\n",
        "\n",
        "print(\"\\nPerformance with and without Firefox (%, 1 decimal):\")\n",
        "display(results_projects_pct)"
      ],
      "metadata": {
        "id": "-Rw3zq59x4rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Win/loss summary (SBERT tuned vs BM25S, using existing ranks)"
      ],
      "metadata": {
        "id": "R9MErBfpuqYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert rank lists to numpy arrays\n",
        "bm25_ranks = np.array(bm25_high, dtype=float)\n",
        "sbert_base_ranks = np.array(sbert_high, dtype=float)\n",
        "sbert_tuned_ranks = np.array(tuned_high, dtype=float)\n",
        "\n",
        "# Treat \"no hit\" (np.inf) as a very bad rank for comparison\n",
        "BIG = 1e9\n",
        "bm25_eff = np.where(np.isfinite(bm25_ranks), bm25_ranks, BIG)\n",
        "sbert_base_eff = np.where(np.isfinite(sbert_base_ranks), sbert_base_ranks, BIG)\n",
        "sbert_tuned_eff = np.where(np.isfinite(sbert_tuned_ranks), sbert_tuned_ranks, BIG)\n",
        "\n",
        "n_queries = len(bm25_ranks)\n",
        "\n",
        "# Win/loss masks for tuned SBERT vs BM25\n",
        "mask_sbert_tuned_better = sbert_tuned_eff < bm25_eff\n",
        "mask_bm25_better = bm25_eff < sbert_tuned_eff\n",
        "mask_tied = sbert_tuned_eff == bm25_eff\n",
        "\n",
        "count_sbert_tuned_better = int(mask_sbert_tuned_better.sum())\n",
        "count_bm25_better = int(mask_bm25_better.sum())\n",
        "count_tied = int(mask_tied.sum())\n",
        "\n",
        "summary_winloss = pd.DataFrame(\n",
        "    {\n",
        "        \"count\": [\n",
        "            count_sbert_tuned_better,\n",
        "            count_bm25_better,\n",
        "            count_tied,\n",
        "        ],\n",
        "        \"percent\": [\n",
        "            100.0 * count_sbert_tuned_better / n_queries,\n",
        "            100.0 * count_bm25_better / n_queries,\n",
        "            100.0 * count_tied / n_queries,\n",
        "        ],\n",
        "    },\n",
        "    index=[\n",
        "        \"SBERT tuned better\",\n",
        "        \"BM25S better\",\n",
        "        \"Tie / both similar\",\n",
        "    ],\n",
        ").round(1)\n",
        "\n",
        "print(\"Win/loss comparison between SBERT (tuned) and BM25S for Queries in high-support groups:\")\n",
        "display(summary_winloss)\n"
      ],
      "metadata": {
        "id": "exH43xLbuunw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "winloss_masks = {\n",
        "    \"sbert_tuned_better\": mask_sbert_tuned_better,\n",
        "    \"bm25_better\": mask_bm25_better,\n",
        "    \"tie\": mask_tied,\n",
        "}"
      ],
      "metadata": {
        "id": "BW9la_oIu0pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_group_arr = train_df[\"dup_group\"].to_numpy()\n",
        "# train_issue_ids = train_df[\"issue_id\"].to_numpy()\n",
        "# train_texts = train_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "\n",
        "target_group_size = 10  # (= 1 anchor + 9 duplicates)\n",
        "group_sizes = (\n",
        "    train_df.dropna(subset=[\"dup_group\"])\n",
        "            .groupby(\"dup_group\")[\"issue_id\"]\n",
        "            .count()\n",
        ")\n",
        "\n",
        "# Find all query indices in qset whose group size == 10\n",
        "q_group_sizes = qset[\"dup_group\"].map(group_sizes)\n",
        "candidate_indices = q_group_sizes[q_group_sizes == target_group_size].index.tolist()\n",
        "\n",
        "example_idx = min(candidate_indices)   # pick the first — deterministic\n",
        "\n",
        "row = qset.loc[example_idx]\n",
        "\n",
        "print(\"=== Selected Query Example ===\")\n",
        "print(\"Query index:\", example_idx)\n",
        "print(\"Project:\", row[\"project\"])\n",
        "print(\"Duplicate group:\", row[\"dup_group\"])\n",
        "print(\"Group size:\", target_group_size)\n",
        "print(\"\\nQuery text:\\n\", row[\"text\"][:400].replace(\"\\n\", \" \"))\n",
        "\n",
        "# All true duplicate IDs in this group\n",
        "true_dup_ids = train_df.loc[\n",
        "    train_df[\"dup_group\"] == row[\"dup_group\"],\n",
        "    \"issue_id\"\n",
        "].tolist()\n",
        "\n",
        "print(\"\\nTrue duplicate issue IDs in this group:\")\n",
        "print(true_dup_ids)\n",
        "\n",
        "dup_texts = train_df.loc[train_df[\"issue_id\"].isin(true_dup_ids), \"text\"].fillna(\"\").astype(str).tolist()\n",
        "print(\"\\nExample duplicate ticket texts:\")\n",
        "for i, t in enumerate(dup_texts[4:8]):   # show few examples\n",
        "    print(f\"\\nDuplicate {i+1}:\\n{t[:300].replace('\\n', ' ')}\")"
      ],
      "metadata": {
        "id": "vrUvbPNIu4iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_bm25  = ranks_overall_bm25[example_idx]\n",
        "rank_base  = ranks_overall_sbert[example_idx]\n",
        "rank_tuned = ranks_overall_tuned[example_idx]\n",
        "\n",
        "print(\"\\n=== Precomputed Ranks (from earlier cells) ===\")\n",
        "print(f\"BM25S rank:        {rank_bm25}\")\n",
        "print(f\"SBERT base rank:   {rank_base}\")\n",
        "print(f\"SBERT tuned rank:  {rank_tuned}\")\n",
        "\n",
        "# Determine winner based on tuned SBERT vs BM25\n",
        "if rank_tuned < rank_bm25:\n",
        "    print(\"\\nWinner for this query: SBERT tuned\")\n",
        "elif rank_bm25 < rank_tuned:\n",
        "    print(\"\\nWinner for this query: BM25S\")\n",
        "else:\n",
        "    print(\"\\nWinner for this query: Tie\")"
      ],
      "metadata": {
        "id": "NFkoFuQlu9D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute top-k retrieved docs for BM25 and SBERT-tuned\n",
        "\n",
        "k = 10\n",
        "query_text = row[\"text\"]\n",
        "print(\"Query text:\\n\", query_text)\n",
        "\n",
        "# BM25 retrieval\n",
        "tokens = query_text.lower().split()\n",
        "bm25_scores = np.array(bm25_retriever.get_scores(tokens))\n",
        "order_bm25 = np.argsort(-bm25_scores)\n",
        "\n",
        "# SBERT tuned retrieval\n",
        "q_emb = tuned_query_embs[example_idx]\n",
        "sbert_scores = tuned_train_embs @ q_emb\n",
        "order_sbert = np.argsort(-sbert_scores)\n",
        "\n",
        "# Pre-extracted arrays for convenience\n",
        "train_ids = train_df[\"issue_id\"].astype(int).tolist()\n",
        "train_groups = train_df[\"dup_group\"].astype(float).tolist()\n",
        "train_texts = train_df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "true_group = float(row[\"dup_group\"])\n",
        "\n",
        "# Helper to print tables cleanly\n",
        "def print_ranked(label, order, scores):\n",
        "    print(f\"\\nTop {k} results — {label}\")\n",
        "    for r in range(k):\n",
        "        doc = order[r]\n",
        "        issue_id = train_ids[doc]\n",
        "        group_val = float(train_groups[doc])\n",
        "        is_dup = (group_val == true_group)\n",
        "        mark = \"<-- TRUE DUP\" if is_dup else \"\"\n",
        "        snippet = train_texts[doc][:150].replace(\"\\n\", \" \")\n",
        "        print(f\"{r+1:2d}. id={issue_id}  {snippet}  {mark}\")\n",
        "\n",
        "# Determine first true-duplicate rank\n",
        "def first_dup(order):\n",
        "    for r, doc in enumerate(order):\n",
        "        if float(train_groups[doc]) == true_group:\n",
        "            return r + 1\n",
        "    return None\n",
        "\n",
        "rank_bm25 = first_dup(order_bm25)\n",
        "rank_tuned = first_dup(order_sbert)\n",
        "\n",
        "#Print results\n",
        "print_ranked(\"BM25\", order_bm25, bm25_scores)\n",
        "print(f\"\\nBM25 first true-duplicate rank = {rank_bm25}\")\n",
        "\n",
        "print_ranked(\"SBERT tuned\", order_sbert, sbert_scores)\n",
        "print(f\"\\nSBERT tuned first true-duplicate rank = {rank_tuned}\")\n",
        "\n",
        "#Show a couple of TRUE duplicate texts\n",
        "print(\"\\n=== Example duplicate tickets' texts ===\")\n",
        "for tid in true_dup_ids[4:8]:       # just few duplicates\n",
        "    doc = train_ids.index(tid)\n",
        "    text_snip = train_texts[doc][:400].replace(\"\\n\", \" \")\n",
        "    print(f\"\\nDuplicate id={tid}:\")\n",
        "    print(text_snip)\n"
      ],
      "metadata": {
        "id": "f56ee4eSu_NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Project-wise comparison\n",
        "\n",
        "bm25_all = np.array(ranks_overall_bm25, dtype=float)\n",
        "tuned_all = np.array(ranks_overall_tuned, dtype=float)\n",
        "\n",
        "projects = sorted(qset[\"project\"].unique().tolist())\n",
        "rows = []\n",
        "\n",
        "for p in projects:\n",
        "    mask = (qset[\"project\"] == p).to_numpy()\n",
        "\n",
        "    bm25_proj = bm25_all[mask].tolist()\n",
        "    tuned_proj = tuned_all[mask].tolist()\n",
        "\n",
        "    bm25_stats = summarize(bm25_proj)\n",
        "    tuned_stats = summarize(tuned_proj)\n",
        "\n",
        "    # Extract only R@10 + MRR\n",
        "    bm25_r10 = bm25_stats[\"R@10\"]\n",
        "    bm25_mrr = bm25_stats[\"MRR\"]\n",
        "    tuned_r10 = tuned_stats[\"R@10\"]\n",
        "    tuned_mrr = tuned_stats[\"MRR\"]\n",
        "\n",
        "    # Determine winner\n",
        "    if tuned_mrr > bm25_mrr:\n",
        "        winner = \"SBERT tuned\"\n",
        "    elif bm25_mrr > tuned_mrr:\n",
        "        winner = \"BM25S\"\n",
        "    else:\n",
        "        winner = \"Tie\"\n",
        "\n",
        "    rows.append({\n",
        "        \"project\": p,\n",
        "        \"BM25S_R@10 (%)\": bm25_r10 * 100,\n",
        "        \"BM25S_MRR (%)\": bm25_mrr * 100,\n",
        "        \"SBERT_tuned_R@10 (%)\": tuned_r10 * 100,\n",
        "        \"SBERT_tuned_MRR (%)\": tuned_mrr * 100,\n",
        "        \"Winner_by_MRR\": winner,\n",
        "    })\n",
        "\n",
        "df_proj_pct = (\n",
        "    pd.DataFrame(rows)\n",
        "      .set_index(\"project\")\n",
        "      .round(1)\n",
        ")\n",
        "\n",
        "print(\"\\nPer-project comparison:\")\n",
        "display(df_proj_pct)\n"
      ],
      "metadata": {
        "id": "u5lsUs8lyYJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0GpJ7sAydLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COBLiTAIzGtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45yErzG4zHAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRWh2QNWzHR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "138ybs9HzHqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}